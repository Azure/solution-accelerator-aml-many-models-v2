{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeatable, Parallel Model Training and Model Tracking and Analysis  \n",
    "  \n",
    "In this notebook, you will use Azure Machine learning to create a repeatable, scaleable prediction framework. Key concepts in this notebook are:  \n",
    "  \n",
    "1. **Create and Run Inference Pipeline**\n",
    "   - The [model inference script](../src/parallel_inference.py) is custom and interchangeable so it can be easily adapted for changing needs\n",
    "   - The same compute cluster, and thus parallel and scaleable concepts, from [Step 1 - Model Training](./1_training_pipeline.ipynb) are used\n",
    "   - The inference script uses MLFlow to automatically select and use the \"best\" model version from a given partitions registered models\n",
    "     - For this notebook, the lowest test RMSE score is how \"best\" is defined\n",
    "  \n",
    "1. **Azure Machine Learning Endpoint / Deployment**\n",
    "   -  You will also deploy the prediction pipeline to an Azure Machine Learning Batch Endpoint, this allows for:\n",
    "      -  Scheduling or a programatic method to run the pipeline to generate predictions\n",
    "      -  Deployment of multiple pipeline to the same endpoint for ease or conducting side-by-side comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml import MLClient, Input, Output, load_component\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities import BatchEndpoint, PipelineComponentBatchDeployment, Data\n",
    "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
    "from azure.ai.ml.parallel import parallel_run_function, RunFunction\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ML Client\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "# authenticate\n",
    "credential = DefaultAzureCredential(tenantid=os.environ.get('TENANT_ID'))\n",
    "\n",
    "# Get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id = os.environ.get('SUBSCRIPTION_ID'),\n",
    "    resource_group_name = os.environ.get('RESOURCE_GROUP_NAME'),\n",
    "    workspace_name = os.environ.get('WORKSPACE_NAME'),\n",
    ")\n",
    "\n",
    "print(dict(os.environ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Environment\n",
    "custom_env_name = \"mm-remote-env-py310\"\n",
    "\n",
    "# Define Compute Target\n",
    "cpu_compute_target = \"mm-cpu-cluster\"\n",
    "\n",
    "# Define mlflow tracking URI\n",
    "azureml_tracking_uri = ml_client.workspaces.get(ml_client.workspace_name).mlflow_tracking_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acess data asset\n",
    "data_name = \"oj-sim-sales-test\"\n",
    "test_data_asset = ml_client.data.get(data_name, label='latest')\n",
    "print(test_data_asset.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data preparation component\n",
    "partition_data = load_component(source=\"../src/components/partition_data/partition_data.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parallel inference step\n",
    "# Declare parallel job, with a run_function task\n",
    "many_model_inference_with_partition_keys = parallel_run_function(\n",
    "    name=\"distributed_inference\",\n",
    "    display_name=\"Many Model Predictions\",\n",
    "    description=\"parallel job to batch predict with many models\",\n",
    "    inputs=dict(\n",
    "        data_source=Input(\n",
    "            type=AssetTypes.MLTABLE,\n",
    "            description=\"Input mltable with predefined partition format.\",\n",
    "            mode=InputOutputModes.DIRECT,  # [Important] To use 'partition_keys', input MLTable is required to use 'direct' mode.\n",
    "        ),\n",
    "        drop_cols=Input(\n",
    "            type=\"string\",\n",
    "            description=\"Columns need to be dropped before training. Split by comma.\",\n",
    "        ),\n",
    "        date_col=Input(\n",
    "            type=\"string\",\n",
    "            description=\"Name of date column in data\",\n",
    "        ),\n",
    "        tracking_uri=Input(\n",
    "            type=\"string\",\n",
    "            description=\"tracking uri of mlflow server (aml workspace)\",\n",
    "        ),\n",
    "    ),\n",
    "    outputs=dict(\n",
    "        output_dir=Output(\n",
    "            type=AssetTypes.URI_FOLDER,\n",
    "            mode=InputOutputModes.RW_MOUNT,\n",
    "        )\n",
    "    ),\n",
    "    input_data=\"${{inputs.data_source}}\",  # Define which input data will be splitted into mini-batches\n",
    "    partition_keys=[\"Store\",\"Brand\"],  # Use 'partition_keys' as the data division method. This method requires MLTable input with partition setting pre-defined in MLTable artifact.\n",
    "    instance_count=6,  # Use X nodes from compute cluster to run this parallel job.\n",
    "    max_concurrency_per_instance=2,  # Create 2 worker processors in each compute node to execute mini-batches.\n",
    "    error_threshold=-1,  # Monitor the failures of item processed by the gap between mini-batch input count and returns. 'Many model training' scenario doesn't fit this setting and '-1' means ignore counting failure items by mini-batch returns.\n",
    "    mini_batch_error_threshold=5,  # Monitor the failed mini-batch by exception, time out, or null return. When failed mini-batch count is higher than this setting, the parallel job will be marked as 'failed'.\n",
    "    retry_settings=dict(\n",
    "        max_retries=1,  # Define how many retries when mini-batch execution is failed by exception, time out, or null return.\n",
    "        timeout=60,  # Define the timeout in second for each mini-batch execution.\n",
    "    ),\n",
    "    logging_level=\"DEBUG\", # DEBUG, INFO, WARNING, ERROR, ETC\n",
    "    task=RunFunction(\n",
    "        code=\"../src/\",\n",
    "        entry_script=\"parallel_inference.py\",\n",
    "        environment=ml_client.environments.get(custom_env_name, label=\"latest\"),\n",
    "        program_arguments=\"--drop_cols ${{inputs.drop_cols}} \"  # Passthrough input parameters into parallel_train script.\n",
    "        \"--tracking_uri ${{inputs.tracking_uri}} \"\n",
    "        \"--date_col ${{inputs.date_col}} \"\n",
    "        \"--output_dir ${{outputs.output_dir}} \",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Pipeline\n",
    "# Declare the overall input of the job.\n",
    "test_oj_data = Input(\n",
    "    path=test_data_asset.path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    mode=InputOutputModes.RO_MOUNT,\n",
    ")\n",
    "\n",
    "# Declare pipeline structure.\n",
    "@pipeline(display_name=\"inference pipeline\")\n",
    "def parallel_inference_pipeline(pipeline_input_data: Input(type=AssetTypes.MLTABLE)):\n",
    "    \n",
    "    # Declare 1st data partition command job.\n",
    "    partition_job = partition_data(\n",
    "        data_source=pipeline_input_data,\n",
    "        partition_keys=\"Store,Brand\",\n",
    "    )\n",
    "\n",
    "    # Declare 2nd parallel model training job.\n",
    "    parallel_inference = many_model_inference_with_partition_keys(\n",
    "        data_source=partition_job.outputs.tabular_output_data,\n",
    "        drop_cols=\"Advert,Store,Brand\",\n",
    "        date_col=\"WeekStarting\",\n",
    "        tracking_uri=azureml_tracking_uri\n",
    "    )\n",
    "    \n",
    "    return {\"pipeline_output\": parallel_inference.outputs.output_dir}\n",
    "\n",
    "    # User could override parallel job run-level property when invoke that parallel job/component in pipeline.\n",
    "    # parallel_train.resources.instance_count = 5\n",
    "    # parallel_train.max_concurrency_per_instance = 2\n",
    "    # parallel_train.mini_batch_error_threshold = 10\n",
    "\n",
    "# Create pipeline instance\n",
    "inference_pipeline = parallel_inference_pipeline(pipeline_input_data=test_oj_data,)\n",
    "\n",
    "# Set pipeline level compute\n",
    "inference_pipeline.settings.default_compute = cpu_compute_target\n",
    "print(inference_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Inference Pipeline (optional)\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    inference_pipeline,\n",
    "    experiment_name=\"many-models-parallel-inference-job\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy as a Batch Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Batch Endpoint\n",
    "\n",
    "endpoint_name = \"manymodels-batch-endp\"\n",
    "print(f\"Endpoint name: {endpoint_name}\")\n",
    "\n",
    "endpoint = BatchEndpoint(\n",
    "    name=endpoint_name,\n",
    "    description=\"A many models batch inference endpoint\",\n",
    ")\n",
    "\n",
    "try:\n",
    "    ml_client.batch_endpoints.get(endpoint_name)\n",
    "    print(f\"'{endpoint_name}' endpoint already exists. Will re-use existing endpoint\")\n",
    "except Exception as e:\n",
    "    print(\"No existing endpoint found. Creating new endpoint....\")\n",
    "    ml_client.batch_endpoints.begin_create_or_update(endpoint).result()\n",
    "    print(\"Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pipeline Component\n",
    "pipeline_component = parallel_inference_pipeline().component\n",
    "\n",
    "# Register Pipeline Component for better tracking and versioning\n",
    "ml_client.components.create_or_update(pipeline_component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Deployment\n",
    "deployment_name = \"mm-inference-deployment\"\n",
    "\n",
    "deployment = PipelineComponentBatchDeployment(\n",
    "    name=deployment_name,\n",
    "    description=\"A many models deployment.\",\n",
    "    endpoint_name=endpoint_name,\n",
    "    component=pipeline_component,\n",
    "    settings={\"default_compute\": cpu_compute_target},\n",
    ")\n",
    "\n",
    "try:\n",
    "    ml_client.batch_deployments.get(name=deployment_name, endpoint_name=endpoint_name)\n",
    "    print(f\"'{deployment_name}' already exists. Will re-use existing.\")\n",
    "except Exception as e:\n",
    "    print(\"No existing deployment found. Creating new deployment....\")\n",
    "    ml_client.batch_deployments.begin_create_or_update(deployment).result()\n",
    "    print(\"Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke Batch endpoint\n",
    "input_data = test_oj_data\n",
    "\n",
    "endp_job = ml_client.batch_endpoints.invoke(\n",
    "    endpoint_name=endpoint_name,\n",
    "    deployment_name=deployment_name,\n",
    "    inputs={\"pipeline_input_data\": input_data},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.jobs.stream(name=endp_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Outputs ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download output data\n",
    "local_download_path = \"../data/oj_sim_sales/outputs/\"\n",
    "ml_client.jobs.download(name=endp_job.name, download_path=local_download_path, output_name=\"pipeline_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read output data as pandas df\n",
    "\n",
    "glob_path = os.path.join(local_download_path +\"named-outputs/pipeline_output/*.csv\")\n",
    "output_files = glob.glob(glob_path)\n",
    "output_df = pd.concat((pd.read_csv(f) for f in output_files))\n",
    "\n",
    "display(len(output_df))\n",
    "display(output_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register Output as a AML Data Asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register predictions as a data asset\n",
    "\n",
    "predictions_csv_path = \"../data/oj_sim_sales/outputs/consolidated_outputs.csv\"\n",
    "\n",
    "output_df.to_csv(predictions_csv_path, index=False)\n",
    "\n",
    "# set the version number of the data asset to the current UTC time\n",
    "v = time.strftime(\"%Y.%m.%d.%H%M%S\", time.gmtime())\n",
    "local_path = \"../data/oj_sim_sales/\"\n",
    "\n",
    "inference_output = Data(\n",
    "    name=\"oj-sim-sales-predictions\",\n",
    "    version=v,\n",
    "    description=\"Lab 02 output file\",\n",
    "    path=predictions_csv_path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    ")\n",
    "\n",
    "ml_client.data.create_or_update(inference_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "many_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
